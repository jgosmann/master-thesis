\documentclass[11pt,a4paper,twoside,BCOR=15mm]{scrbook}

%\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
%\usepackage{scrpage2}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[style=authoryear,backend=biber,language=american]{biblatex}
\usepackage{bm}
%\usepackage{booktabs}
\usepackage{commath}
\usepackage{enumerate}
%\usepackage[flushmargin,ragged]{footmisc}
\usepackage{gitinfo}
\usepackage[draft]{graphicx}
%\usepackage{ifthen}
%\usepackage{hhline}
%\usepackage{lastpage}
%\usepackage{listings}
\usepackage{nag}
\usepackage[refpage,intoc]{nomencl}
\usepackage{longtable}
\usepackage{polyglossia}
%\usepackage{ngerman}
\usepackage{siunitx}
\usepackage{scrpage2}
\usepackage{subfigure}
\usepackage{upgreek}
%\usepackage{xcolor}
%\usepackage{xr-hyper}
\usepackage{hyperref} % should be loaded last
\usepackage[acronym,nogroupskip,nonumberlist,toc]{glossaries} % needs to be loaded after hyperref

\newglossarystyle{lltable}{%
    \renewenvironment{theglossary}{\begin{longtable}[l]{lp{10.5cm}}}{\end{longtable}}%
    \renewcommand*{\glossaryheader}{}%
    \renewcommand*{\glsgroupheading}[1]{}%
    \renewcommand*{\glsgroupskip}{}%
    \renewcommand*{\glossentry}[2]{\glossentryname{##1}&\glossentrydesc{##1}\\}%
    \renewcommand*{\subglossentry}[3]{\glossentry{##2}{##3}}%
}
\setglossarystyle{lltable}
\makeglossaries{}

\setdefaultlanguage{english}
\setotherlanguages{german}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

%\lstset{basicstyle=\fontfamily{pcr}\selectfont, keywordstyle=\bfseries, language=C++}
\sisetup{per-mode=symbol}

\pagestyle{scrheadings}
%\automark[section]{section}
%\clearscrheadfoot{}
%\ofoot{\pagemark}
%\ifoot{\headmark}
% FIXME remove git hash for final version
\ifoot{\gitCommitterIsoDate{} \gitHash}
%\ohead{}
%\deftripstyle{mychapter}{}{}{}{}{}{\pagemark}
%\renewcommand*{\chapterpagestyle}{mychapter}

\newcommand{\vc}[1]{\bm{#1}}
\newcommand{\vcc}[1]{\textbf{#1}}
\newcommand{\mat}[1]{\bm{#1}}
\newcommand{\Tr}{^{\top}}
\newcommand{\e}{\mathrm{e}}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator*{\mslim}{mslim}

\newcommand{\ped}[1]{_{\mathrm{#1}}}

\newcommand{\newterm}[1]{\emph{#1}}
\newcommand{\abbrev}[2]{#1 (#2\newacronym{#2}{#2}{#1})\glsadd{#2}}
\newcommand{\pabbrev}[2]{#1s (#2\newacronym{#2}{#2}{#1})\glsadd{#2}}
\newcommand{\newtermAbbrev}[2]{\newterm{#1} (#2\newacronym{#2}{#2}{#1})\glsadd{#2}}

\newacronym{GNFSSSV}{G-NF-SS-SV}{Gaussian, noise free, single source, single 
    vehicle scenario}
\newacronym{DNFSSSV}{D-NF-SS-SV}{Gaussian dispersion, noise free, single source, 
    single vehicle scenario}
\newacronym{DNFMSSV}{D-NF-MS-SV}{Gaussian dispersion, noise free, multiple 
    source, single vehicle scenario}
\newacronym{DSNSSSV}{D-NF-SS-SV}{Gaussian dispersion, sensor noise, single 
    source, single vehicle scenario}
\newacronym{DSNMSSV}{D-NF-MS-SV}{Gaussian dispersion, sensor noise, multiple 
    source, single vehicle scenario}
\newacronym{DSNMSMV}{D-NF-MS-MV}{Gaussian dispersion, sensor noise, multiple 
    source, multiple vehicle scenario}
\newacronym{EU}{EU}{European Union}
\glsadd{GNFSSSV}
\glsadd{DNFSSSV}
\glsadd{DNFMSSV}
\glsadd{DSNSSSV}
\glsadd{DSNMSSV}
\glsadd{DSNMSMV}
\glsadd{EU}

\addbibresource{references.bib}

\title{Gaussian Processes for Plume Distribution Estimation with UAVs}
\author{Jan Gosmann}
\makeatletter
\hypersetup{
  pdftitle={\@title},
  pdfauthor={\@author}
}
\makeatother

\newcommand{\addsym}[3]{\newglossaryentry{#1}{sort={#1},name={\ensuremath{#2}},description={#3}}\glsadd{#1}}
\newcommand{\addspecialsym}[4]{\newglossaryentry{#1}{sort={#2},name={\ensuremath{#3}},description={#4}}\glsadd{#1}}
\addspecialsym{sim}{~}{\sim}{distributed according to}
\addsym{0}{\vc{0}}{null vector $(0, \dots, 0)\Tr$}
\addsym{AT}{\mat{A}\Tr}{the transpose of $\mat A$}
\addsym{det}{\det{}\mat{A}}{the determinant of $\mat A$}
\addsym{diagx}{
    \diag\vc{x}}{diagonal matrix with the elements of the vector $\vc x$}
\addsym{diagA}{
    \diag\mat{A}}{vector with the diagonal elements of the matrix $\mat A$}
\addsym{Ncal}{\mathcal{N} (\vc{m}, \mat{\Sigma})}{
    multivariate normal distribution with mean $\vc m$ and covariance matrix 
    $\mat \Sigma$}
\addsym{p}{p (x)}{probability or probability density of $x$}
\addsym{xi}{x_i}{$i$-th component of the vector $\vc x$}
\addsym{xy}{x | y}{conditional random variable $x$ given $y$}
\addspecialsym{Gamma}{Γ}{\Gamma(\nu)}{Gamma function}
\addsym{Kvz}{K_{\nu} (z)}{modified Bessel function}
\addsym{A}{\norm{\mat{A}}}{norm of $\mat A$}
\addsym{x}{\abs{\vc{x}}}{Euclidian ($L^2$) norm of $\vc x$}
\addsym{N}{N (x; \mu, \sigma^2)}{
    Gaussian probability density at $x$ with mean $\mu$ and variance $\sigma^2$}
\addspecialsym{Phi}{Φ}{\Phi(x; \mu, \sigma^2)}{
    Gaussian cummulative distribution function at $x$ with mean $\mu$ and 
    variance $\sigma^2$}

\begin{document}
\frontmatter
\maketitle % TODO title page

\thispagestyle{empty}
\begin{german}
    \vspace*{\fill}
    \noindent Hiemit erkläre ich, dass ich die vorliegende Arbeit selbstständig 
    und eigenhändig sowie ohne unerlaubte fremde Hilfe und ausschließlich unter 
    Verwendung der aufgeführten Quellen und Hilfsmittel angefertigt habe.

    \vspace{\intextsep}
    \noindent Berlin, den \today

    \vspace{\intextsep}
    \vspace{\intextsep}
    \noindent Jan Gosmann
    \vspace*{\fill}
    \vspace*{\fill}
\end{german}

\cleardoublepage\thispagestyle{empty}
\vspace*{\fill}
\section*{\abstractname}
% TODO
Lorem ipsum dolor sit amet, consectetur adipisici elit, sed eiusmod tempor 
incidunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis 
nostrud exercitation ullamco laboris nisi ut aliquid ex ea commodi 
consequat. Quis aute iure reprehenderit in voluptate velit esse cillum 
dolore eu fugiat nulla pariatur. Excepteur sint obcaecat cupiditat non 
proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

\begin{german}
\section*{\abstractname}
% TODO
Lorem ipsum dolor sit amet, consectetur adipisici elit, sed eiusmod tempor 
incidunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis 
nostrud exercitation ullamco laboris nisi ut aliquid ex ea commodi 
consequat. Quis aute iure reprehenderit in voluptate velit esse cillum 
dolore eu fugiat nulla pariatur. Excepteur sint obcaecat cupiditat non 
proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
\end{german}
\vspace*{\fill}

\tableofcontents

\setglossarypreamble{Variables are typeset in italics, whereas constants are 
    upright.  Vectors and matrices use a bold font. Additionally, matrices use 
    uppercase letters.}

\printglossary[title={Symbols and Notation}]

\printglossary[type=\acronymtype]

\mainmatter{}
\include{intro}
\include{scenarios}
\include{gp}
\include{error}
\include{tech}
\include{exp}

\chapter{Outlook Time-varying Plumes}

\chapter{Conclusion}

\appendix
\chapter{Samples Needed to Decrease Noise to a Certain 
    Degree}\label{sec:decnoise}

\chapter{Sparse Online Gaussian Processes}\label{sec:sparse-gp-apdx}
In the following it will be proven that the matrix $-\mat C_t$ of a sparse 
online Gaussian process \parencite{Csato:2002fp} is symmetric, positive 
definite. The proof allows to relate the rule for full updates to the update of 
the inverse Cholesky factor in Chapter~\ref{sec:onlineup}. As the notation by 
\textcite{Csato:2002fp} differs it should be noted that $[\sigma_x^2] = \mat B$ 
and $\vc k_{t+1} = K(X, {\vc x_{t+1}})$.

\begin{theorem}
    The matrix $-\mat C_t$ is symmetric, positive definite for all $t \geq 1$.
\end{theorem}

%\begin{theorem}
%Applying the sparse online Gaussian Process full update rule 
%\parencite[equation~2.9]{Csato:2002fp} on a symmetric, positive definite matrix 
%$-\mat C_t = \del[0]{\mat L^{-1}}\Tr \mat L^{-1}$ the resulting matrix $-\mat 
%C_{t+1}$ is also symmetric, positive definite.\end{theorem}

\begin{proof}
    The proof is done by induction. It has to be shown
    \begin{itemize}
        \item that $-\mat C_1$ (base case) fulfills the proposition
        \item and that $-\mat C_{t+1}$ fulfills the proposition given it is 
            fulfilled for $\mat C_t$ (inductive step).
    \end{itemize}
    The deletion of a basis vector has not to be considered as it exactly undoes 
    a full update and then performs a reduced update.

    \paragraph{Base Case}
    For $t = 1$ we obtain
    \begin{equation*}
        -\mat C_1 = \sbr{r^{(t+1)}} = \sbr{\sigma_x^-2}
    \end{equation*}
    As $\sigma_x > 0$ it follows that $-\mat C_1$ is symmetric positive 
    definite.

    \paragraph{Inductive Step}
    For showing the symmetry and positive definiteness after a full update it 
    suffices to show that Cholesky factorization for the updated matrix $-\mat 
    C_{t+1} = \del[0]{\mat L'^{-1}}\Tr \mat L'^{-1}$ exists.
\begin{align*}
        -\mat C_{t+1} &= -U_{t+1}(\mat C_t) - r^{(t+1)} \vc s_{t+1} \vc 
        s_{t+1}\Tr \\
        &= \sbr{\begin{array}{cc}
                \mat -C_t + \sigma_x^{-2} \mat C_t \vc k_{t+1} \vc k_{t+1}\Tr 
                \mat C_t\Tr & \sigma_x^{-2} \mat C_t \vc k_{t+1} \vc\e_{t+1}\Tr 
                \\
                \sigma_x^{-2} \vc\e_{t+1} k_{t+1}\Tr \mat C_t\Tr & \sigma_x^{-2}
            \end{array}} \\
        &= \sbr{\begin{array}{cc}
                \del[0]{\mat L^{-1}}\Tr & \sigma_x^{-1} \mat C_t \vc k_{t+1} \\
                0 & \sigma_x^{-1}
            \end{array}} \sbr{\begin{array}{cc}
                \mat L^{-1} & 0 \\
                \sigma_x^{-1} \vc k_{t+1}\Tr \mat C_t\Tr & \sigma_x^{-1}
            \end{array}} \\
        &= \del[0]{\mat L'^{-1}}\Tr \mat L'^{-1}
    \end{align*}

    In case of a reduced update the relation
    \begin{equation*}
        -\mat C_{t+1} = -\mat C_t - r^{(t+1)} \vc s_{t+1} \vc s_{t+1}\Tr
    \end{equation*}
    holds. The term $- r^{(t+1)} \vc s_{t+1} \vc s_{t+1}\Tr$ is symmetric, 
    positive definite as it is an outer vector product (which is symmetric, 
    positive definite) multiplied by a positive number $-r^{(t+1)} 
    = \sigma_x^{-2} > 0$. The sum $-\mat C_{t+1}$ of symmetric, positive 
    definite matrices is again symmetric, positive definite.
\end{proof}

\begin{corollary}
    A full update of a symmetric, positive-definite matrix $-\mat{C}_t$ as 
    formulated by \textcite[equation~2.9]{Csato:2002fp} consists of the same 
    calculations as an online update of the inverse Cholesky factor in 
    Equation~\ref{eqn:invChol}.
\end{corollary}
This is obvious from the inductive step for a full update.

%\section{TODO title}

%\begin{theorem}
%Assuming Gaussian likelihood the matrix $\mat C_t$ in the sparse online Gaussian 
%process formulation by \textcite{Csato:2002fp} can be expressed as $\mat C_t 
%= -\tilde{\mat K}_t^{-1} - \mat R_{\mat C,t}$. Herein, $\tilde{\mat K}_t$ is the 
%covariance matrix of $t$ basis vectors.
%\end{theorem}

%\begin{proof}
%TODO alpha
%The proof is based on induction and it has to be shown
%\begin{itemize}
    %\item that the relation holds for $\mat C_1$ (base case)
    %\item and that the relation holds after performing an update of any kind 
        %given that the relation holds for one $t$ (inductive step).
%\end{itemize}

%\subsection{Base Case}
%For $t = 1$ we obtain
%\begin{align*}
    %\mat C_1 &= r^{(1)} \vc s_1 \vc s_1\Tr \\
    %&= - \frac{1}{\sigma\ped{n}^2 + k(\vc x_1, \vc x_1)} \vc\e_1 \vc\e_1\Tr \\
    %&= -\tilde{\mat K}_t^{-1} \text{.}
%\end{align*}
%By setting $\mat R_{\mat C, t} = \sbr{0}$ the base case holds.

%\subsection{Inductive Step}
%For both,
%\begin{itemize}
    %\item a full update,
    %\item and a reduced update,
%\end{itemize}
%it has to be shown that the relation holds for $\mat C_{t+1}$ given that it 
%holds for $\mat C_t$. Also, it has to be shown that the relation  
%$\hat{\mat{C}}_t = - \tilde{\mat K}_t^{-1} - \hat{\mat R}_{\mat C, t}$ holds 
%after deleting the last added basis vector $t+1$ given that the relation holds 
%for $\mat C_t$.

%\subsubsection{Full Update}
%With
%\begin{equation*}
    %\mat\varDelta = \sbr{\begin{array}{cc}
                %\del[1]{\tilde{\mat K}_t^{-1}}\Tr \vc k_{t+1} \del{\mat 
                    %B^{-1}}\Tr \mat B^{-1} \vc k_{t+1}\Tr \tilde{\mat K}_t^{-1} 
                %& \del[1]{\tilde{\mat K}_t^{-1}}\Tr \vc k_{t+1} 
                %\del{\mat{B}^{-1}}\Tr \mat B^{-1} \\
                %\del{\mat B^{-1}}\Tr \mat B^{-1} \vc k_{t+1}\Tr 
                %\tilde{\mat{K}}_t^{-1} & \del{\mat B^{-1}}\Tr \mat B^{-1}
            %\end{array}}
%\end{equation*}
%performing a full update as described in Chapter~\ref{sec:onlineup} gives
%\begin{align*}
    %\tilde{\mat K}_{t+1}^{-1} &= \del{\mat L_{t+1}\Tr}^{-1} \mat L_{t+1}^{-1} \\
    %&= U_{t+1}\!\del[1]{\tilde{\mat K}_t^{-1}} + \mat \varDelta \\
    %&= U_{t+1}\!\del[1]{-\mat C_t - \mat R_{\mat C, t}} + \mat \varDelta \\
    %&= -U_{t+1}\!\del[1]{\mat C_t} - r^{(t+1)} \vc s_{t+1} \vc s_{t+1}\Tr 
    %- U_{t+1}\!\del[1]{\mat R_{\mat C, t}} + \mat \varDelta + r^{(t+1)} \vc 
    %s_{t+1} \vc s_{t+1}\Tr \\
    %&= -\mat C_{t+1} - U_{t+1}\!\del[1]{\mat R_{\mat C, t}} + \mat \varDelta 
    %+ r^{(t+1)} \vc s_{t+1} \vc s_{t+1}\Tr \text{.}
%\end{align*}
%By setting
%\begin{equation*}
    %\mat R_{\mat C, t+1} = -\mat \varDelta - r^{(t+1)} \vc s_{t+1} \vc 
    %s_{t+1}\Tr
%\end{equation*}
%the relation $\mat C_{t+1} = - \tilde{\mat K}_{t+1}^{-1} - \mat R_{\mat C, t+1}$ 
%is obtained.

%\subsubsection{Reduced Update}
%For a reduced update the relation holds as
%\begin{align*}
    %\mat C_{t+1} &= \mat C_t + r^{(t+1)} \vc s_{t+1} \vc s_{t+1}\Tr \\
    %&= -\tilde{\mat K}_t^{-1} - \mat R_{\mat C, t} + r^{(t+1)} \vc s_{t+1} \vc 
    %s_{t+1}\Tr \\
    %&= -\tilde{\mat K}_t^{-1} - \mat R_{\mat C, t+1}
%\end{align*}
%with
%\begin{equation*}
    %R_{\mat C, t+1} = \mat R_{\mat C, t} - r^{(t+1)} \vc s_{t+1} \vc s_{t+1}\Tr 
    %\text{.}
%\end{equation*}

%\subsubsection{Deletion of a Basis Vector}
%As in \textcite{Csato:2002fp} it is assumed that the basis vector to delete was 
%just added. If this is not the case, all matrices and vectors can be rearranged 
%to bring the basis vector in the last position.

%Using Equation~\ref{eqn:invChol} one obtains
%\begin{align*}
    %\tilde{\mat K}_{t+1}^{-1} &= \del[1]{\mat L_{t+1}^{-1}}\Tr \mat L_{t+1}^{-1} 
    %\\
    %&= \sbr{\begin{array}{cc}
            %\del[1]{\mat L_t^{-1}}\Tr \mat L_t^{-1} + \frac{1}{q^*} \mat Q^* 
            %{\mat Q^*}\Tr  & \mat Q^* \\
            %{\mat Q^*}\Tr & q^*
        %\end{array}} \\
%\intertext{with}
    %\mat Q^* &= \tilde{\mat K}_t^{-1} \vc k_{t+1} \del[1]{\mat B^{-1}}\Tr \mat 
    %B^{-1} \\
    %q^* &= \del[1]{\mat B^{-1}}\Tr \mat B^{-1} \text{.}
%\end{align*}
%Denoting a matrix $\mat A$ with the last row and column removed as 
%${(\mat{A})}_-$ it follows
%\begin{align*}
    %\del[1]{\tilde{\mat K}_{t+1}^{-1}}_- &= \del[1]{\mat L_t^{-1}}\Tr \mat 
    %L_t^{-1} + \frac{1}{q^*} \mat Q^* {\mat Q^*}\Tr \\
     %&= \tilde{\mat K}_t^{-1} + \frac{1}{q^*} \mat Q^* {\mat Q^*}\Tr
%\end{align*}
%With this the following holds for the deletion of an basis vector:
%\begin{align*}
    %\hat{\mat C} &= \del{\mat C_{t+1}}_- + c^* \frac{\mat Q^* {\mat 
            %Q^*}\Tr}{q^{*2}} - \frac{1}{q^*} \del{\mat Q^* {\mat C^*}\Tr + \mat 
        %C^* {\mat Q^*}\Tr} \\
     %&= -\del{\tilde{\mat K}_{t+1}^{-1}}_- - \del{\mat R_{\mat C, t+1}}_- + c^* 
     %\frac{\mat Q^* {\mat Q^*}\Tr}{q^{*2}} - \frac{1}{q^*} \del{\mat Q^* {\mat 
             %C^*}\Tr + \mat C^* {\mat Q^*}\Tr} \\
     %&= -\tilde{\mat K}_{t}^{-1} - \del{\mat R_{\mat C, t+1}}_- + c^* \frac{\mat 
         %Q^* {\mat Q^*}\Tr}{q^{*2}} - \frac{1}{q^*} \del{\mat Q^* {\mat Q^*}\Tr 
         %+ \mat Q^* {\mat C^*}\Tr + \mat C^* {\mat Q^*}\Tr} \\
     %&= -\tilde{\mat K}_{t}^{-1} - \hat{\mat R}_{\mat C, t} \\
%\intertext{with}
    %\hat{\mat R}_{\mat C, t} &= \del{\mat R_{\mat C, t+1}}_- - c^* \frac{\mat 
        %Q^* {\mat Q^*}\Tr}{q^{*2}} + \frac{1}{q^*} \del{\mat Q^* {\mat Q^*}\Tr 
        %+ \mat Q^* {\mat C^*}\Tr + \mat C^* {\mat Q^*}\Tr}
%\end{align*}

%\end{proof}

%\section{Downdating with the Inverse Cholesky Factor}\label{sec:downdating}
%As in \textcite{Csato:2002fp} it is assumed that the basis vector to delete was 
%just added. If this is not the case, all matrices and vectors can be rearranged 
%to bring the basis vector in the last position.

%\begin{theorem}
    %The downdated inverse of $\mat K = \mat L \mat L\Tr$ is given by $D\del{\mat 
        %K}^{-1} = D\del{\mat L^{-1}}\Tr \mat L^{-1}$ wherein $D(\mat A)$ is the 
        %matrix $\mat A$ downdated by removing the last column and row.
%\end{theorem}

\chapter{PDUCB Differentiability}\label{sec:pducb-diff}

\chapter{Prior Width}\label{sec:prior}

\backmatter{}
\listoffigures{}
\printbibliography{}

\end{document}
